{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69ad6b5",
   "metadata": {},
   "source": [
    "# *FoML - Assignment 1*\n",
    "## *Yashas Tadikamalla - AI20BTECH11027*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e27ec",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "### a) Describe what happens to training error (using all available data) when neighbor size k varies from n to 1\n",
    "\n",
    "Let us consider (1-accuracy) as the evaluation metric and assume that we always give one particular class as output if a tie occurs.\n",
    "\n",
    "Training error for k=1.\n",
    "Clearly, for the case of 1 nearest neighbour, training error is zero. This is because, the nearest neighbour for any training data point is that point itself. Hence, the prediction will match the actual value for every training data point. Consequently, (1-accuracy) will turn out to be 0.\n",
    "\n",
    "Training error for k=n.\n",
    "For every point, out of the k(=n) nearest neighbours, there are n/2 neighbours of one class, and n/2 of the other. So, from the assumption, the accuracy will be 1/2, and hence, (1-accuracy) will be 1/2.\n",
    "\n",
    "For k in \\[2,n-1\\], depending on the data, the training error may increase or decrease, but overall, it will increase from 0 to 1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994119ee",
   "metadata": {},
   "source": [
    "### b) Predict and explain with a sketch how the generalization error (e.g. holding out some data for testing) would change when k varies? Explain your reasoning\n",
    "\n",
    "Let us consider (1-accuracy) as the evaluation metric and assume that we always give one particular class as output if a tie occurs.\n",
    "\n",
    "Interestingly, k=1, despite 0 training error, would a lead high generalisation error. i.e, it will be an overfitting model, as it would get influenced by noisy training measurements. As k increases, the influence of noise reduces, and generalisation error decreases, till k attains an optimal value. Then, on further increasing k, the generalisation error again rises, as the model would start to get influenced by neighbours of other class. \n",
    "\n",
    "The following sketch gives a **\"rough\"** estimate of how the errors change with k\n",
    "![1b_sketch](1b_sketch.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a992e80a",
   "metadata": {},
   "source": [
    "### c) Give two reasons (with sound justification) why k-NN may be undesirable when input dimension is high.\n",
    "\n",
    "As the input dimension increases, \n",
    "   1. Calculating the norm becomes very tedious. Norm calculation has a linear time complexity (Calculation of euclidean norm requires 3n flops). Hence, with increase in n, the computation cost also increases very quickly. \n",
    "   2. The data suffers from curse of dimensionality. The k-NN model primarily depends on the fact that two points are similar based on the distance between them. But, as the number of dimensions increase, the pairwise distance between the points coverge to a value. Hence, the model cannot perform/train well on the data. This aspect of curse of dimensionality is referred to as 'distance concentration'.\n",
    "   3. In addition to the time complexity discussed in point 1, with the increase in number of attributes, the space complexity also increases, as k-NN stores all its training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f03c2",
   "metadata": {},
   "source": [
    "### d) Is it possible to build a univariate decision tree (with decisions at each node of the form “is x > a”, “is x < b”, “is y > c”, or “is y < d” for any real constants a, b, c, d) which classifies exactly similar to a 1-NN using the Euclidean distance measure? If so, explain how. If not, explain why not.\n",
    "\n",
    "**No**.\n",
    "We know than, a voronoi diagram depicts the decision boundaries of a 1-NN using Euclidean distance measure. These boundaries need not be parallel to the coordinate axes(i.e, they can be at some angle). But for a univariate decision tree, these boundaries are invariably parallel to the x or y axes. Hence, we cannot build a univariate decision tree which classifies exactly similar to a 1-NN.\n",
    "\n",
    "Ex voronoi diagram\n",
    "![1d_voronoi](1d_voronoi.jpeg)\n",
    "\n",
    "Ex decision tree boundary diagram\n",
    "![1d_decisiontree](1d_decisiontree.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef3b1e",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "### a) A training set consists of one dimensional examples from two classes. The training examples from class 1 are {0.5, 0.1, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.35, 0.25} and from class 2 are {0.9, 0.8, 0.75, 1.0}. Fit a (one dimensional) Gaussian using Maximum Likelihood to each of these two classes. You can assume that the variance for class 1 is 0.0149, and the variance for class 2 is 0.0092. Also estimate the class probabilities $p_1$ and $p_2$ using Maximum Likelihood. What is the probability that the test point x = 0.6 belongs to class 1?\n",
    "\n",
    "Let $c_1,c_2$ denote classes 1,2 respectively. We need to fit a gaussian using maximum likelihood to the following data:\n",
    "\n",
    "   1. {0.5, 0.1, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.35, 0.25} of $c_1$.\n",
    "   \n",
    "      Given, ${\\sigma_1}^2=0.0149$\n",
    "      $$\\mu_1=\\dfrac{0.5+0.1+0.2+0.4+0.3+0.2+0.2+0.1+0.35+0.25}{10}=0.26$$\n",
    "\n",
    "      From Maximum likelihood, \n",
    "      $$p(x|c_1)=\\dfrac{1}{\\sqrt{2\\pi{\\sigma_1}^2}}e^{-\\dfrac{1}{2}\\left(\\dfrac{x-\\mu_1}{\\sigma_1}\\right)^2}$$\n",
    "\n",
    "   2. {0.9, 0.8, 0.75, 1.0} of $c_2$.\n",
    "   \n",
    "      Given, ${\\sigma_2}^2=0.0092$\n",
    "      $$\\mu_2=\\dfrac{0.9+0.8+0.75+1.0}{4}=0.8625$$\n",
    "\n",
    "      From Maximum likelihood, \n",
    "      $$p(x|c_2)=\\dfrac{1}{\\sqrt{2\\pi{\\sigma_2}^2}}e^{-\\dfrac{1}{2}\\left(\\dfrac{x-\\mu_2}{\\sigma_2}\\right)^2}$$\n",
    "\n",
    "From given training data,\n",
    "$$p_1=\\dfrac{10}{14}=\\dfrac{5}{7}=0.7142$$\n",
    "$$p_2=\\dfrac{4}{14}=\\dfrac{2}{7}=0.2857$$\n",
    "\n",
    "From Bayes theorem\n",
    "$$p(c_i|x)=\\dfrac{p(x|c_i)p_i}{\\sum_{j=1}^{J}p(x|c_j)p_j}$$\n",
    "\n",
    "Therefore, $p(c_1/x=0.6)$ is\n",
    "$$p(c_1|x=0.6)=\\dfrac{p(x=0.6|c_1)p_1}{\\sum_{j=1}^{2}p(x=0.6|c_j)p_j}$$\n",
    "$$p(c_1|x=0.6)=\\dfrac{\\dfrac{1}{\\sqrt{2\\pi{\\sigma_1}^2}}e^{-\\dfrac{1}{2}\\left(\\dfrac{0.6-\\mu_1}{\\sigma_1}\\right)^2}\\times p_1}{\\sum_{j=1}^{2}\\dfrac{1}{\\sqrt{2\\pi{\\sigma_j}^2}}e^{-\\dfrac{1}{2}\\left(\\dfrac{0.6-\\mu_j}{\\sigma_j}\\right)^2}\\times p_j}$$\n",
    "$$p(c_1|x=0.6)=\\dfrac{\\dfrac{1}{\\sqrt{2\\pi{0.0149}^2}}e^{-\\dfrac{1}{2}\\left(\\dfrac{0.6-0.26}{0.0149}\\right)^2}\\times 0.7142}{\\dfrac{1}{\\sqrt{2\\pi{0.0149}^2}}e^{-\\dfrac{1}{2}\\left(\\dfrac{0.6-0.26}{0.0149}\\right)^2}\\times 0.7142+\\dfrac{1}{\\sqrt{2\\pi{0.0092}^2}}e^{-\\dfrac{1}{2}\\left(\\dfrac{0.6-0.8625}{0.0092}\\right)^2}\\times 0.2857}$$\n",
    "$$\\therefore p(c_1|x=0.6)=0.6305$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7513c",
   "metadata": {},
   "source": [
    "### b) You are now going to make a text classifier. To begin with, you attempt to classify documents as either sport or politics. You decide to represent each document as a (row) vector of attributes describing the presence or absence of the following words. x = (goal,football,golf,defence,offence,wicket,office,strategy) Training data from sport documents and from politics documents is represented below in a matrix in which each row represents the 8 attributes.\n",
    "\n",
    "$$x_{politics}=\\begin{bmatrix}\n",
    "                      1 & 0 & 1 & 1 & 1 & 0 & 1 & 1\\\\\n",
    "                      0 & 0 & 0 & 1 & 0 & 0 & 1 & 1\\\\\n",
    "                      1 & 0 & 0 & 1 & 1 & 0 & 1 & 0\\\\\n",
    "                      0 & 1 & 0 & 0 & 1 & 1 & 1 & 1\\\\\n",
    "                      0 & 0 & 0 & 1 & 1 & 0 & 1 & 1\\\\\n",
    "                      0 & 0 & 0 & 1 & 1 & 0 & 0 & 1\\\\\n",
    "                \\end{bmatrix}$$\n",
    "                \n",
    "$$x_{sport}=\\begin{bmatrix}\n",
    "                      1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "                      0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "                      1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n",
    "                      1 & 1 & 0 & 1 & 0 & 0 & 0 & 1\\\\\n",
    "                      1 & 1 & 0 & 1 & 1 & 0 & 0 & 0\\\\\n",
    "                      0 & 0 & 0 & 1 & 0 & 1 & 0 & 0\\\\\n",
    "                \\end{bmatrix}$$\n",
    "      \n",
    "### Using a maximum likelihood naive Bayes classifier, what is the probability that the document x = (1, 0, 0, 1, 1, 1, 1, 0) is about politics?\n",
    "\n",
    "Given, 8 attributes, 16 training data. If $x$ denotes an instance of training data, its attributes are denoted by $x_1,x_2,\\dots,x_8$. \n",
    "\n",
    "Using statistics of training data,\n",
    "\n",
    "$$p(politics)=p(sport)=\\dfrac{1}{2}$$\n",
    "\n",
    "From Bayes theorem\n",
    "$$p(c_i|x)=\\dfrac{p(x|c_i)p_i}{\\sum_{j=1}^{J}p(x|c_j)p_j}$$\n",
    "$$p(politics|x)=\\dfrac{p(x|politics)p(politics)}{p(x|politics)p(politics)+p(x|sport)p(sport)}$$\n",
    "\n",
    "Using maximum likelihood naive Bayes classifier\n",
    "$$p(x|politics)=p(x_1|politics)p(x_2|politics)\\dots p(x_7|politics)p(x_8|politics)$$\n",
    "$$p(x|sport)=p(x_1|sport)p(x_2|sport)\\dots p(x_7|sport)p(x_8|sport)$$\n",
    "\n",
    "Look up table of required probabilities:\n",
    "$$p(x_1|politics)=p(1|politics)=\\dfrac{2}{6},p(x_1|sport)=p(1|sports)=\\dfrac{4}{6}$$\n",
    "$$p(x_2|politics)=p(0|politics)=\\dfrac{5}{6},p(x_2|sport)=p(0|sports)=\\dfrac{2}{6}$$\n",
    "$$p(x_3|politics)=p(0|politics)=\\dfrac{5}{6},p(x_3|sport)=p(0|sports)=\\dfrac{5}{6}$$\n",
    "$$p(x_4|politics)=p(1|politics)=\\dfrac{5}{6},p(x_4|sport)=p(1|sports)=\\dfrac{4}{6}$$\n",
    "$$p(x_5|politics)=p(1|politics)=\\dfrac{5}{6},p(x_5|sport)=p(1|sports)=\\dfrac{1}{6}$$\n",
    "$$p(x_6|politics)=p(1|politics)=\\dfrac{1}{6},p(x_6|sport)=p(1|sports)=\\dfrac{1}{6}$$\n",
    "$$p(x_7|politics)=p(1|politics)=\\dfrac{5}{6},p(x_7|sport)=p(1|sports)=\\dfrac{0}{6}$$\n",
    "$$p(x_8|politics)=p(0|politics)=\\dfrac{5}{6},p(x_8|sport)=p(0|sports)=\\dfrac{5}{6}$$\n",
    "\n",
    "So, the required probability is\n",
    "$$p(politics|x)=\\dfrac{p(politics)p(x_1|politics)p(x_2|politics)\\dots p(x_7|politics)p(x_8|politics)}{p(politics)p(x_1|politics)p(x_2|politics)\\dots p(x_7|politics)p(x_8|politics)+p(sport)p(x_1|sport)p(x_2|sport)\\dots p(x_7|sport)p(x_8|sport)}$$\n",
    "$$p(politics|x)=\\dfrac{\\dfrac{1}{2}\\dfrac{2}{6}\\dfrac{5}{6}\\dots \\dfrac{5}{6}\\dfrac{5}{6}}{\\dfrac{1}{2}\\dfrac{2}{6}\\dfrac{5}{6}\\dots \\dfrac{5}{6}\\dfrac{5}{6}+\\dfrac{1}{2}\\dfrac{4}{6}\\dfrac{2}{6}\\dots \\dfrac{0}{6}\\dfrac{5}{6}}$$\n",
    "$$p(politics|x)=\\dfrac{\\dfrac{1}{2}\\dfrac{2}{6}\\dfrac{5}{6}\\dots \\dfrac{5}{6}\\dfrac{5}{6}}{\\dfrac{1}{2}\\dfrac{2}{6}\\dfrac{5}{6}\\dots \\dfrac{5}{6}\\dfrac{5}{6}+0}$$\n",
    "$$p(politics|x)=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f79cb",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "### a) Decision Tree Implementation: Implement your own version of the decision tree using binary univariate split, entropy and information gain.\n",
    "### b) Evaluate your decision tree using 10-fold cross validation. Please see lecture slides for details. In a nutshell, you will first make a split of the provided data into 10 parts. Then hold out 1 part as the test set and use the remaining 9 parts for training. Train your decision tree using the training set and use the trained decision tree to classify entries in the test set. Repeat this process for all 10 parts, so that each entry will be used as the test set exactly once. To get the final accuracy value, take the average of the 10 folds’ accuracies. With correct implementation of both parts (decision tree and cross validation), your classification accuracy should be around 0.78 or higher.\n",
    "### c) Improvement Strategies: Now, try and improve your decision tree algorithm. Some things you could do include (not exhaustive):\n",
    "   1. **Use Gini index instead of entropy**\n",
    "   2. **Use multi-way split (instead of binary split)**\n",
    "   3. **Use multivariate split (instead of univariate)**\n",
    "   4. **Prune the tree after splitting for better generalization**\n",
    "   \n",
    "### Report your performance as an outcome of ANY TWO improved strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38be9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy-fold 1 : 0.7939\n",
      "accuracy-fold 2 : 0.8265\n",
      "accuracy-fold 3 : 0.7857\n",
      "accuracy-fold 4 : 0.8061\n",
      "accuracy-fold 5 : 0.8265\n",
      "accuracy-fold 6 : 0.8143\n",
      "accuracy-fold 7 : 0.8245\n",
      "accuracy-fold 8 : 0.8102\n",
      "accuracy-fold 9 : 0.8323\n",
      "accuracy-fold 10 : 0.8160\n",
      "Cross validation Accuracy: 0.8136\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import copy\n",
    "from math import *\n",
    "\n",
    "# My Name \n",
    "myname = \"Yashas Tadikamalla\" \n",
    "\n",
    "# Implementing decision tree \n",
    "class DecisionTree():\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, training_set):   \n",
    "        \n",
    "        # attribute refers to the attribute used or splitting\n",
    "        # threshold refers to the value of attribute  \n",
    "        tree = {'attribute':None,'threshold':None,'left_subtree':None,'right_subtree':None,'prediction':None} \n",
    "                  \n",
    "        # storing the length of the training set\n",
    "        length=len(training_set)  \n",
    "        \n",
    "        # it is a binary classification problem\n",
    "        class0=0\n",
    "        class1=0\n",
    "        for i in range(0,length):\n",
    "            if(int(training_set[i][11])==0):\n",
    "                class0+=1\n",
    "            else:\n",
    "                class1+=1\n",
    "\n",
    "        # calculating impurity at the given node\n",
    "        p0=(class0)/(class0+class1)        \n",
    "\n",
    "        # if the purity is 1, then we have reached a leaf node\n",
    "        if(p0==0):\n",
    "            tree['prediction']=1\n",
    "            return tree        \n",
    "        elif(p0==1):\n",
    "            tree['prediction']=0\n",
    "            return tree\n",
    "\n",
    "        # finding Minimum and Maximum values of each attribute which ocurred in the training data\n",
    "        Min_val=[1000]*11\n",
    "        Max_val=[-1]*11\n",
    "        for i in range (0,11):\n",
    "            for j in range(0,length):\n",
    "                if(training_set[j][i]>Max_val[i]):\n",
    "                    Max_val[i]=training_set[j][i]\n",
    "                if(training_set[j][i]<Min_val[i]):\n",
    "                    Min_val[i]=training_set[j][i]\n",
    "        \n",
    "        # storing possible thresholds in a 2D list\n",
    "        th=[]\n",
    "        no_of_th = 30\n",
    "        for i in range (0,11):\n",
    "            a=Min_val[i]\n",
    "            b=Max_val[i]\n",
    "            th.append(np.linspace(a+(b-a)/(no_of_th+1),a+((b-a)*no_of_th)/(no_of_th+1),no_of_th))        \n",
    "        \n",
    "        # using entropy as the measure of impurity\n",
    "        imp_p=-(p0*log(p0))-((1-p0)*log(1-p0))\n",
    "\n",
    "        # calculating information gain for various attributes and their values\n",
    "        ig=[[-1000 for i in range(no_of_th)] for j in range(11)]\n",
    "        Max_ig=-100\n",
    "        I=-1\n",
    "        \n",
    "        for i in range (0,11):            \n",
    "            for j in range (0,no_of_th):\n",
    "                left_tree=[]\n",
    "                right_tree=[]\n",
    "                for k in range (0,length):\n",
    "                    if(training_set[k][i]>=th[i][j]):\n",
    "                        right_tree.append(training_set[k])\n",
    "                    else:\n",
    "                        left_tree.append(training_set[k])\n",
    "\n",
    "                # probabilities at left subtree\n",
    "                l_class0=0\n",
    "                l_class1=0\n",
    "                l_length=len(left_tree)\n",
    "                \n",
    "                # incase for a th[i][j], the data doesn't split into two trees, we avoid such th[i][j]\n",
    "                if(l_length==0 or l_length==length):\n",
    "                    continue\n",
    "                    \n",
    "                for k in range (0,l_length):\n",
    "                    if(int(left_tree[k][11])==0):\n",
    "                        l_class0+=1\n",
    "                    elif(int(left_tree[k][11])==1):\n",
    "                        l_class1+=1                        \n",
    "                \n",
    "                l_p0=(l_class0)/(l_class0+l_class1)\n",
    "                if(l_p0==0 or l_p0==1):\n",
    "                    l_imp = 0\n",
    "                else:\n",
    "                    l_imp = (-(l_p0*log(l_p0))-((1-l_p0)*log(1-l_p0)))*(l_length/length)\n",
    "                    \n",
    "                # probabilities at right subtree\n",
    "                r_class0=0\n",
    "                r_class1=0\n",
    "                r_length=length-len(left_tree)\n",
    "                \n",
    "                for k in range (0,r_length):\n",
    "                    if(int(right_tree[k][11])==0):\n",
    "                        r_class0+=1\n",
    "                    elif(int(right_tree[k][11])==1):\n",
    "                        r_class1+=1\n",
    "\n",
    "                r_p0=(r_class0)/(r_class0+r_class1)\n",
    "                if(r_p0==0 or r_p0==1):\n",
    "                    r_imp = 0\n",
    "                else:\n",
    "                    r_imp = (-(r_p0*log(r_p0))-((1-r_p0)*log(1-r_p0)))*(r_length/length)\n",
    "                                    \n",
    "                # weighed impurity of children nodes\n",
    "                imp=l_imp+r_imp\n",
    "\n",
    "                # information gain\n",
    "                ig[i][j]=imp-imp_p\n",
    "                if(ig[i][j]>Max_ig):\n",
    "                    Max_ig=ig[i][j]\n",
    "                    \n",
    "                    # storing the attribute and threshold\n",
    "                    I=i\n",
    "                    J=th[I][j]\n",
    "                        \n",
    "            \n",
    "        # Updating the tree\n",
    "        tree['attribute']=I\n",
    "        tree['threshold']=J\n",
    "        \n",
    "        # splitting the data on optimal attribute I, and threshold value J\n",
    "        left_data=[]\n",
    "        right_data=[]\n",
    "        for k in range (0,length):\n",
    "            if(training_set[k][I]>=J):\n",
    "                right_data.append(training_set[k])\n",
    "            else:\n",
    "                left_data.append(training_set[k])\n",
    "\n",
    "        # recursively training left and right subtrees \n",
    "        left_subtree={}\n",
    "        right_subtree={}\n",
    "        left_subtree = DecisionTree.learn(self,left_data)\n",
    "        right_subtree = DecisionTree.learn(self,right_data)\n",
    "        \n",
    "        tree['left_subtree']=left_subtree\n",
    "        tree['right_subtree']=right_subtree        \n",
    "        \n",
    "        self.tree = tree\n",
    "        return tree\n",
    "    \n",
    "    def classify(self,tree,test_instance):\n",
    "        \n",
    "        if(tree['prediction']!=None):\n",
    "            result = tree['prediction']\n",
    "            return result\n",
    "        else:\n",
    "            if(test_instance[tree['attribute']]>=tree['threshold']):\n",
    "                return self.classify(tree['right_subtree'],test_instance)\n",
    "            else:\n",
    "                return self.classify(tree['left_subtree'],test_instance)        \n",
    "        \n",
    "\n",
    "def run_decision_tree():\n",
    "\n",
    "    # Loading dataset\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [list(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    \n",
    "    # typecasting data for easier calculations\n",
    "    data = np.array(data,dtype=float)\n",
    "    \n",
    "    # k-fold cross validation\n",
    "    Acc=0\n",
    "    for h in range(0,10):\n",
    "        \n",
    "        # Split training/test sets\n",
    "        K = 10\n",
    "        training_set = [x for i, x in enumerate(data) if i % K != h]\n",
    "        test_set = [x for i, x in enumerate(data) if i % K == h]\n",
    "\n",
    "        Tree = DecisionTree()\n",
    "        # Construct a tree using training set\n",
    "        tree=Tree.learn(training_set)\n",
    "\n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = Tree.classify(tree,instance[:-1])\n",
    "            results.append( result == instance[-1])\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        Acc+=accuracy\n",
    "        print(\"accuracy-fold\",h+1,\": %.4f\" % accuracy)       \n",
    "    \n",
    "    print(\"Cross validation Accuracy: %.4f\" % (Acc/10))\n",
    "    # Writing results to a file \n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % (Acc/10))\n",
    "    f.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20af2e",
   "metadata": {},
   "source": [
    "# Improvement strategies used:\n",
    "   1. **Gini Index:** \n",
    "   Using Gini index over entropy improves the accuracy, but very slightly. \n",
    "    \n",
    "   2. **Pre-Pruning:**\n",
    "   This helps in improving accuracy and generalisation ability of the tree. The logic can be explained as follows: Suppose a node gets a set of points such that, p(class 0) is very close to 0(or 1). This means most of the points are of class 1(or class 0), and very few are from the other class. These minority points might be some noise or outliers in the data, which need to be handled. Instead of setting a hard rule of p(class 0)=0(or 1), we can give a range of \\[0,0.05\\](or \\[0.95,1\\]). This way, we can address such data and avoid overfitting. Another advantage of pre-pruning is that the algorithm is faster, as it does not go till the leaf level.\n",
    "    \n",
    "   3. **reducing no_of_th:**\n",
    "   This action seemed to improve accuracy considerably. no_of_th is the number of threshold values which I am checking for a given attribute, to find an optimal threshold for splitting data based on that attribute. Initially, when I ran my code for no_of_th=30, my code gave accuracy in folds between 78%-82%. Overall cross validation accuracy was also around 80%. But when I reduced this number, the accuracy improved consistenly. For no_of_th=10, some folds even touch 85%-86% accuracy. I think this happens because, when we try to increase no_of_th, we are overfitting the threshold values for which, data points split into subtrees. By taking lesser no_of_th, we can get a better idea of the splitting threshold values of attributes, and hence, generalise better. Reducing no_of_th also reduces the time taken by the code, as it now has to check 1/3 (if we reduce it from 30 to 10) of the values as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34aaba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy-fold 1 : 0.8408\n",
      "accuracy-fold 2 : 0.8592\n",
      "accuracy-fold 3 : 0.8102\n",
      "accuracy-fold 4 : 0.8184\n",
      "accuracy-fold 5 : 0.8469\n",
      "accuracy-fold 6 : 0.8102\n",
      "accuracy-fold 7 : 0.8327\n",
      "accuracy-fold 8 : 0.8327\n",
      "accuracy-fold 9 : 0.7975\n",
      "accuracy-fold 10 : 0.8078\n",
      "Cross validation Accuracy: 0.8256\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import copy\n",
    "from math import *\n",
    "\n",
    "# My Name \n",
    "myname = \"Yashas Tadikamalla\" \n",
    "\n",
    "# Implementing decision tree \n",
    "class DecisionTree():\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, training_set):   \n",
    "        \n",
    "        # attribute refers to the attribute used or splitting\n",
    "        # threshold refers to the value of attribute  \n",
    "        tree = {'attribute':None,'threshold':None,'left_subtree':None,'right_subtree':None,'prediction':None} \n",
    "                  \n",
    "        # storing the length of the training set\n",
    "        length=len(training_set)  \n",
    "        \n",
    "        # it is a binary classification problem\n",
    "        class0=0\n",
    "        class1=0\n",
    "        for i in range(0,length):\n",
    "            if(int(training_set[i][11])==0):\n",
    "                class0+=1\n",
    "            else:\n",
    "                class1+=1\n",
    "\n",
    "        # calculating impurity at the given node\n",
    "        p0=(class0)/(class0+class1)        \n",
    "\n",
    "        ##### Strategy #####\n",
    "        # pruning the subtrees, with probabilities close to 0 or 1\n",
    "        if(p0>=0 and p0 <=0.05):\n",
    "            tree['prediction']=1\n",
    "            return tree        \n",
    "        elif(p0<=1 and p0>=0.95):\n",
    "            tree['prediction']=0\n",
    "            return tree\n",
    "\n",
    "        # finding Minimum and Maximum values of each attribute which ocurred in the training data\n",
    "        Min_val=[1000]*11\n",
    "        Max_val=[-1]*11\n",
    "        for i in range (0,11):\n",
    "            for j in range(0,length):\n",
    "                if(training_set[j][i]>Max_val[i]):\n",
    "                    Max_val[i]=training_set[j][i]\n",
    "                if(training_set[j][i]<Min_val[i]):\n",
    "                    Min_val[i]=training_set[j][i]\n",
    "        \n",
    "        # storing possible thresholds in a 2D list\n",
    "        th=[]\n",
    "        \n",
    "        ##### Strategy #####\n",
    "        # reducing the no_of_th to improve generalisation\n",
    "        no_of_th = 10\n",
    "        for i in range (0,11):\n",
    "            a=Min_val[i]\n",
    "            b=Max_val[i]\n",
    "            th.append(np.linspace(a+(b-a)/(no_of_th+1),a+((b-a)*no_of_th)/(no_of_th+1),no_of_th))        \n",
    "        \n",
    "        ##### Strategy #####\n",
    "        # using gini as the measure of impurity\n",
    "        imp_p=1-(p0*p0)-((1-p0)*(1-p0))\n",
    "        \n",
    "        # calculating information gain for various attributes and their values\n",
    "        ig=[[-1000 for i in range(no_of_th)] for j in range(11)]\n",
    "        Max_ig=-100\n",
    "        I=-1\n",
    "        \n",
    "        for i in range (0,11):           \n",
    "            for j in range (0,no_of_th):\n",
    "                left_tree=[]\n",
    "                right_tree=[]\n",
    "                for k in range (0,length):\n",
    "                    if(training_set[k][i]>=th[i][j]):\n",
    "                        right_tree.append(training_set[k])\n",
    "                    else:\n",
    "                        left_tree.append(training_set[k])\n",
    "\n",
    "                # probabilities at left subtree\n",
    "                l_class0=0\n",
    "                l_class1=0\n",
    "                l_length=len(left_tree)\n",
    "                \n",
    "                # incase for a th[i][j], the data doesn't split into two trees, we avoid such th[i][j]\n",
    "                if(l_length==0 or l_length==length):\n",
    "                    continue\n",
    "                    \n",
    "                for k in range (0,l_length):\n",
    "                    if(int(left_tree[k][11])==0):\n",
    "                        l_class0+=1\n",
    "                    elif(int(left_tree[k][11])==1):\n",
    "                        l_class1+=1                        \n",
    "                \n",
    "                l_p0=(l_class0)/(l_class0+l_class1)\n",
    "                if(l_p0==0 or l_p0==1):\n",
    "                    l_imp = 0\n",
    "                else:\n",
    "                    l_imp = (1-(l_p0*l_p0)-((1-l_p0)*(1-l_p0)))*(l_length/length)\n",
    "\n",
    "                # probabilities at right subtree\n",
    "                r_class0=0\n",
    "                r_class1=0\n",
    "                r_length=length-len(left_tree)\n",
    "                \n",
    "                for k in range (0,r_length):\n",
    "                    if(int(right_tree[k][11])==0):\n",
    "                        r_class0+=1\n",
    "                    elif(int(right_tree[k][11])==1):\n",
    "                        r_class1+=1\n",
    "\n",
    "                r_p0=(r_class0)/(r_class0+r_class1)\n",
    "                if(r_p0==0 or r_p0==1):\n",
    "                    r_imp = 0\n",
    "                else:\n",
    "                    r_imp = (1-(r_p0*r_p0)-((1-r_p0)*(1-r_p0)))*(r_length/length)\n",
    "                \n",
    "                # weighed impurity of children nodes\n",
    "                imp=l_imp+r_imp\n",
    "\n",
    "                # information gain\n",
    "                ig[i][j]=imp-imp_p\n",
    "                if(ig[i][j]>Max_ig):\n",
    "                    Max_ig=ig[i][j]\n",
    "                    \n",
    "                    # storing the attribute and threshold\n",
    "                    I=i\n",
    "                    J=th[I][j]\n",
    "                        \n",
    "            \n",
    "        # Updating the tree\n",
    "        tree['attribute']=I\n",
    "        tree['threshold']=J\n",
    "        \n",
    "        # splitting the data on optimal attribute I, and threshold value J\n",
    "        left_data=[]\n",
    "        right_data=[]\n",
    "        for k in range (0,length):\n",
    "            if(training_set[k][I]>=J):\n",
    "                right_data.append(training_set[k])\n",
    "            else:\n",
    "                left_data.append(training_set[k])\n",
    "\n",
    "        # recursively training left and right subtrees \n",
    "        left_subtree={}\n",
    "        right_subtree={}\n",
    "        left_subtree = DecisionTree.learn(self,left_data)\n",
    "        right_subtree = DecisionTree.learn(self,right_data)\n",
    "        \n",
    "        tree['left_subtree']=left_subtree\n",
    "        tree['right_subtree']=right_subtree        \n",
    "        \n",
    "        self.tree = tree\n",
    "        return tree\n",
    "    \n",
    "    def classify(self,tree,test_instance):\n",
    "        \n",
    "        if(tree['prediction']!=None):\n",
    "            result = tree['prediction']\n",
    "            return result\n",
    "        else:\n",
    "            if(test_instance[tree['attribute']]>=tree['threshold']):\n",
    "                return self.classify(tree['right_subtree'],test_instance)\n",
    "            else:\n",
    "                return self.classify(tree['left_subtree'],test_instance)        \n",
    "        \n",
    "\n",
    "def run_decision_tree():\n",
    "\n",
    "    # Loading dataset\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [list(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    \n",
    "    # typecasting data for easier calculations\n",
    "    data = np.array(data,dtype=float)\n",
    "    \n",
    "    # k-fold cross validation\n",
    "    Acc=0\n",
    "    for h in range(0,10):\n",
    "        \n",
    "        # Split training/test sets\n",
    "        K = 10\n",
    "        training_set = [x for i, x in enumerate(data) if i % K != h]\n",
    "        test_set = [x for i, x in enumerate(data) if i % K == h]\n",
    "        \n",
    "        Tree = DecisionTree()\n",
    "        # Construct a tree using training set\n",
    "        tree=Tree.learn(training_set)\n",
    "\n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = Tree.classify(tree,instance[:-1])\n",
    "            results.append( result == instance[-1])\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        Acc+=accuracy\n",
    "        print(\"accuracy-fold\",h+1,\": %.4f\" % accuracy)       \n",
    "    \n",
    "    print(\"Cross validation Accuracy: %.4f\" % (Acc/10))\n",
    "    # Writing results to a file \n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % (Acc/10))\n",
    "    f.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
