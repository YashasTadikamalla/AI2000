{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217dcecb",
   "metadata": {},
   "source": [
    "# *FoML - Assignment 2*\n",
    "## *Yashas Tadikamalla - AI20BTECH11027*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbbc20",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "### In the derivation for the Support Vector Machine, we assumed that the margin boundaries are given by w.x+b = +1 and w.x+b = âˆ’1. Show that, if the +1 and -1 on the right-hand side were replaced by some arbitrary constants $+\\gamma$ and $-\\gamma$ where $\\gamma>0$, the solution for the maximum margin hyperplane is unchanged. (You can show this for the hard-margin SVM without any slack variables.)\n",
    "\n",
    "Let us analyse the math involved in the optimisation problem for the case where RHS=1 and for the case where RHS=$\\gamma$ parallely. \n",
    "\n",
    " 1. For RHS=1, the margin length was $\\rho=\\frac{2}{||\\omega||}$, and the optimisation problem was formulated as \n",
    "   $$Minimising \\hspace{0.25cm}\\frac{\\omega.\\omega}{2}, \\forall {(x_i,y_i): y_i(\\omega.x_i+b)\\geq 1}$$\n",
    "\n",
    "    Similarly, for RHS=$\\gamma$, the margin length will be $\\rho=\\frac{2}{||\\omega_\\gamma||}$, and the optimisation problem can be formulated as \n",
    "   $$Minimising \\hspace{0.25cm}\\frac{\\omega_{\\gamma}.\\omega_{\\gamma}}{2}, \\forall {(x_i,y_i): y_i(\\omega_{\\gamma}.x_i+b_{\\gamma})\\geq \\gamma}$$\n",
    "   \n",
    " 2. For RHS=1, the dual problem was \n",
    "   $$max_{\\alpha\\geq 0}min_{\\omega,b}\\frac{1}{2}||\\omega||^2-\\displaystyle\\sum_{i}\\alpha_i[y_i(\\omega.x_i+b)-1]$$\n",
    "    And we obtain Karush-Kuhn-Tucker conditions as\n",
    "   $$\\omega=\\displaystyle\\sum_i\\alpha_iy_ix_i$$\n",
    "   $$\\displaystyle\\sum_i\\alpha_iy_i=0$$\n",
    "    Substituting them in dual problem, we arrive at \n",
    "   $$max_{\\bigg(\\alpha_i\\geq0,\\displaystyle\\sum_i\\alpha_iy_i=0\\bigg)}\\displaystyle\\sum_i\\alpha_i-\\frac{1}{2}\\displaystyle\\sum_{i,j}y_iy_j\\alpha_i\\alpha_j(x_i.x_j)$$\n",
    "    Similarly, for RHS=$\\gamma$, the dual problem is \n",
    "   $$max_{\\alpha_\\gamma\\geq 0}min_{\\omega_\\gamma,b_\\gamma}\\frac{1}{2}||\\omega_\\gamma||^2-\\displaystyle\\sum_{i}\\alpha_{\\gamma_i}[y_i(\\omega_\\gamma.x_i+b_\\gamma)-\\gamma]$$\n",
    "    And we obtain Karush-Kuhn-Tucker conditions as\n",
    "   $$\\omega_\\gamma=\\displaystyle\\sum_i\\alpha_{\\gamma_i}y_ix_i$$\n",
    "   $$\\displaystyle\\sum_i\\alpha_{\\gamma_i}y_i=0$$\n",
    "    Substituting them in dual problem, we arrive at   $$max_{\\bigg(\\alpha_{\\gamma_i}\\geq0,\\displaystyle\\sum_i\\alpha_{\\gamma_i}y_i=0\\bigg)}\\gamma\\displaystyle\\sum_i\\alpha_{\\gamma_i}-\\frac{1}{2}\\displaystyle\\sum_{i,j}y_iy_j\\alpha_{\\gamma_i}\\alpha_{\\gamma_j}(x_i.x_j)$$\n",
    "  This can be written as   $$max_{\\bigg(\\alpha_{\\gamma_i}\\geq0,\\displaystyle\\sum_i\\alpha_{\\gamma_i}y_i=0\\bigg)}\\gamma^2\\bigg(\\displaystyle\\sum_i\\frac{\\alpha_{\\gamma_i}}{\\gamma}-\\frac{1}{2}\\displaystyle\\sum_{i,j}y_iy_j\\frac{\\alpha_{\\gamma_i}}{\\gamma}\\frac{\\alpha_{\\gamma_j}}{\\gamma}(x_i.x_j)\\bigg)$$\n",
    "\n",
    "We can see that, as $\\gamma^2$ is just a constant, comparing the solutions for the first and second dual, we get\n",
    "$$\\alpha_{\\gamma_i}=\\gamma\\alpha_i,\\forall i$$\n",
    "$$\\Rightarrow\\omega_{\\gamma}=\\gamma\\omega,\\because\\omega_\\gamma=\\displaystyle\\sum_i\\alpha_{\\gamma_i}y_ix_i$$\n",
    "$$\\Rightarrow b_{\\gamma}=\\gamma b,\\because y_i(\\omega_{\\gamma}.x_i+b_{\\gamma})\\geq \\gamma$$\n",
    "Hence the solution hyperplane for RHS=1\n",
    "$$\\omega.x+b=0$$\n",
    "and the hyperplanes through support vectors\n",
    "$$\\omega.x+b=\\pm 1$$\n",
    "and the solution hyperplane for RHS=$\\gamma$\n",
    "$$\\gamma(\\omega.x+b)=0\\Rightarrow\\omega.x+b=0$$\n",
    "and the hyperplanes through support vectors\n",
    "$$\\gamma(\\omega.x+b)=\\pm \\gamma\\Rightarrow\\omega.x+b=\\pm 1$$\n",
    "are the same!\n",
    "Hence proved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b05c9",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "### Consider the half-margin of maximum-margin SVM defined by $\\rho$, i.e. $\\rho = \\frac{1}{||w||}$. Show that $\\rho$ is given by\n",
    "$$\\frac{1}{\\rho^2}=\\displaystyle\\sum_{i}^{N}\\alpha_{i}$$\n",
    "### where $\\alpha_{i}$ are the Lagrange multipliers given by the SVM dual\n",
    "\n",
    "We know, for a maximum-margin SVM,\n",
    "$$y_i(\\omega.x_i+b)\\geq1, \\forall i$$\n",
    "where, the equality holds for support vectors, and\n",
    "$$\\alpha_i\\geq0, \\forall i$$\n",
    "where, the equality holds for non-support vectors. This implies, for all the data points,\n",
    "$$\\alpha_i(y_i(\\omega.x_i+b)-1)=0$$\n",
    "$$\\alpha_iy_i(\\omega.x_i)+\\alpha_iy_ib=\\alpha_i,\\forall i$$\n",
    "$$\\omega.(\\alpha_iy_ix_i)+\\alpha_iy_ib=\\alpha_i,\\forall i$$\n",
    "Summing up the equations for each $i$, we get\n",
    "$$\\omega.(\\displaystyle\\sum_i\\alpha_iy_ix_i)+b\\displaystyle\\sum_i\\alpha_iy_i=\\displaystyle\\sum_i\\alpha_i$$\n",
    "Using Karush-Kuhn-Tucker conditions,\n",
    "$$\\omega=\\displaystyle\\sum_i\\alpha_iy_ix_i$$\n",
    "$$\\displaystyle\\sum_i\\alpha_iy_i=0$$\n",
    "Substituting these in the earlier equation, we get\n",
    "$$\\omega.\\omega+b(0)=\\displaystyle\\sum_i\\alpha_i$$\n",
    "$$\\omega^2=\\displaystyle\\sum_i\\alpha_i$$\n",
    "$$||\\omega||^2=\\displaystyle\\sum_i\\alpha_i$$\n",
    "$$\\frac{1}{\\rho^2}=\\displaystyle\\sum_i\\alpha_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106ba61",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "### Let $k_1$ and $k_2$ be valid kernel functions. Comment about the validity of the following kernel functions, and justify your answer with proof or counter-examples as required:\n",
    "### a) $k(x,z)=k_1(x,z)+k_2(x,z)$\n",
    "\n",
    "Mercer's condition for $k(x,z)$ to be a valid kernel function:\n",
    "$$\\int dxdzf(x)k(x,z)f(z)\\geq0$$\n",
    "Given, $k(x,z)=k_1(x,z)+k_2(x,z)$, where $k_1(x,z),k_2(x,z)$ are valid kernel functions. Therefore,\n",
    "$$\\int dxdzf(x)k_1(x,z)f(z)\\geq0$$\n",
    "$$\\int dxdzf(x)k_2(x,z)f(z)\\geq0$$\n",
    "Adding the two equations, we get,\n",
    "$$\\int dxdzf(x)(k_1(x,z)+k_2(x,z))f(z)\\geq0$$\n",
    "$$\\int dxdzf(x)k(x,z)f(z)\\geq0$$\n",
    "Hence, $k(x,z)$ is a valid kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986ae6f",
   "metadata": {},
   "source": [
    "### b) $k(x,z)=k_1(x,z)k_2(x,z)$\n",
    "\n",
    "$k(x,z)$ is said to be kernel function, if it can be decomposed as $k(x,z)=\\phi(x).\\phi(z)$. Given $k_1(x,y),k_2(x,y)$ are valid kernel functions. Hence,\n",
    "$$k_1(x,z)=\\phi_a(x).\\phi_a(z)$$\n",
    "$$k_2(x,z)=\\phi_b(x).\\phi_b(z)$$\n",
    "On multiplying them, we get,\n",
    "$$k_1(x,z)k_2(x,z)=\\displaystyle\\sum_i\\phi_{a_i}(x)\\phi_{a_i}(z)\\displaystyle\\sum_j\\phi_{b_j}(x)\\phi_{b_j}(z)$$\n",
    "$$k_1(x,z)k_2(x,z)=\\displaystyle\\sum_{ij}\\phi_{a_i}(x)\\phi_{b_j}(x)\\phi_{a_i}(z)\\phi_{b_j}(z)$$\n",
    "$$k_1(x,z)k_2(x,z)=\\displaystyle\\sum_{ij}\\phi_{c_{ij}}(x)\\phi_{c_{ij}}(z)$$\n",
    "$$k_1(x,z)k_2(x,z)=\\phi_{c}(x).\\phi_{c}(z)$$\n",
    "$$k(x,z)=\\phi_{c}(x).\\phi_{c}(z)$$\n",
    "Hence, $k(x,z)$ is a valid kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a63c2b",
   "metadata": {},
   "source": [
    "### c) $k(x,z)=h(k_1(x,z))$, where h is a polynomial function with positive coefficients\n",
    "\n",
    "Given $k_1(x,y)$ is a valid kernel function. Using the result in question a, we can deduce that if $k(x,z)$ is valid kernel function, then $ck(x,z)$ is also a valid kernel function $\\forall c>0$. Similarly, using the result in question b, we can deduce that if $k(x,z)$ is valid kernel function, then $k(x,z)^c$ is also a valid kernel function $\\forall c>0$. This implies, if $k(x,z)$ is a valid kernel function, then $c_1k(x,z)^{c_2}$ is a valid kernel function $\\forall c_1,c_2>0$. Hence, it is evident that, $k(x,z)$, which is a polynomial in $k_1(x,z)$ is a valid kernel function, as $k_1(x,z)$ is valid and the coefficients of the polynomial are given to be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7b1a9b",
   "metadata": {},
   "source": [
    "### d) $k(x,z)=exp(k_1(x,z))$\n",
    "\n",
    "Given $k_1(x,y)$ is a valid kernel function. We know, $exp(x)$ can be written as\n",
    "$$exp(x)=\\displaystyle\\lim_{n\\to\\infty}\\bigg(1+x+\\dfrac{x^2}{2!}+\\dfrac{x^3}{3!}+\\dots+\\dfrac{x^n}{n!}\\bigg)$$\n",
    "From result in question c, since $k_1(x,z)$ is a valid kernel function, it is evident that $k(x,z)$ is also valid kernel fuction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb89ea",
   "metadata": {},
   "source": [
    "### e) $k(x,z)=exp\\bigg(\\frac{-||x-z||_2^2}{\\sigma^2}\\bigg)$\n",
    "\n",
    "We can expand $k(x,z)$ as\n",
    "$$k(x,z)=exp\\bigg(\\frac{-||x-z||_2^2}{\\sigma^2}\\bigg)=exp\\bigg(\\frac{-||x||_2^2-||z||_2^2+2x.z}{\\sigma^2}\\bigg)$$\n",
    "$$k(x,z)=exp\\bigg(\\frac{-||x||_2^2}{\\sigma^2}\\bigg)exp\\bigg(\\frac{-||z||_2^2}{\\sigma^2}\\bigg)exp\\bigg(\\frac{2x.z}{\\sigma^2}\\bigg)$$\n",
    "Consider a valid kernel function $k_1(x,z)=\\phi(x).\\phi(z)$, where $\\phi(x)=\\frac{\\sqrt2x}{\\sigma^2}$. Using the result in question d, we get that $exp(k_1(x,z)$ is a valid kernel function. Also, we can use the fact that if\n",
    "$$k(x,z)=g(x)g(y), g:R^n\\to R$$\n",
    "then, $k(x,z)$ is a valid kernel. Hence, using all these results, we can conclude that\n",
    "$$k(x,z)=G(x)G(y)exp\\bigg(\\frac{2x.z}{\\sigma^2}\\bigg)$$\n",
    "is a valid kernel. Infact, it is the gaussian kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac07c83",
   "metadata": {},
   "source": [
    "## Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6800c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# function to extract the required training or testing data\n",
    "def extract(data):\n",
    "    rows=data.shape[0]\n",
    "    req_data=[]\n",
    "    for i in range(rows):\n",
    "        \n",
    "        # we only want the data for 1 or 5\n",
    "        if(int(data[i][0])==1): \n",
    "            data[i][0]=1\n",
    "            req_data.append(data[i])\n",
    "        elif(int(data[i][0])==5):\n",
    "            data[i][0]=-1\n",
    "            req_data.append(data[i])\n",
    "            \n",
    "    req_data=np.array(req_data, dtype=float)\n",
    "    return req_data\n",
    "\n",
    "# Loading training dataset\n",
    "with open(\"hw_train.txt\") as f:\n",
    "    tr_csv = [list(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "\n",
    "# typecasting data for easier calculations\n",
    "tr_csv = np.array(tr_csv,dtype=float)\n",
    "\n",
    "# Loading testing dataset\n",
    "with open(\"hw_test.txt\") as f:\n",
    "    te_csv = [list(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "\n",
    "# typecasting data for easier calculations\n",
    "te_csv = np.array(te_csv,dtype=float)\n",
    "\n",
    "# extracting the required data\n",
    "tr_data=extract(tr_csv).T\n",
    "te_data=extract(te_csv).T\n",
    "\n",
    "# further extracting the features of the data\n",
    "tr_fea=np.array(tr_data[1:],dtype=float)\n",
    "te_fea=np.array(te_data[1:],dtype=float)\n",
    "\n",
    "# further extracting the labels of the data\n",
    "tr_label=np.array(tr_data[0],dtype=int)\n",
    "te_label=np.array(te_data[0],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6039a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On training the model on entire training data\n",
      "The test accuracy of linear SVM is: 0.9787735849056604\n",
      "Number of support vectors: 28\n"
     ]
    }
   ],
   "source": [
    "# training the linear svm\n",
    "lin_svm=SVC(kernel='linear')\n",
    "lin_svm.fit(tr_fea.T,tr_label.T)\n",
    "\n",
    "# using the trained svm on test data\n",
    "prediction=lin_svm.predict(te_fea.T)\n",
    "\n",
    "# calculating the test accuracy\n",
    "Accuracy=accuracy_score(prediction,te_label.T)\n",
    "\n",
    "print(\"On training the model on entire training data\")\n",
    "print(\"The test accuracy of linear SVM is:\",Accuracy)   \n",
    "print(\"Number of support vectors:\",lin_svm.support_vectors_.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85113ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the complete training data in a new variable\n",
    "TR_label=tr_label\n",
    "TR_fea=tr_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00390428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On training the model on first 50 training data points\n",
      "The test accuracy of linear SVM is: 0.9811320754716981\n",
      "Number of support vectors: 2\n",
      "\n",
      "On training the model on first 100 training data points\n",
      "The test accuracy of linear SVM is: 0.9811320754716981\n",
      "Number of support vectors: 4\n",
      "\n",
      "On training the model on first 200 training data points\n",
      "The test accuracy of linear SVM is: 0.9811320754716981\n",
      "Number of support vectors: 8\n",
      "\n",
      "On training the model on first 800 training data points\n",
      "The test accuracy of linear SVM is: 0.9811320754716981\n",
      "Number of support vectors: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training on limited training data\n",
    "limit=[50,100,200,800]\n",
    "for x in limit:\n",
    "    tr_fea=TR_fea.T\n",
    "    tr_fea=tr_fea[:x]\n",
    "    tr_fea=tr_fea.T\n",
    "    tr_label=TR_label.T\n",
    "    tr_label=tr_label[:x]\n",
    "    tr_label=tr_label.T\n",
    "\n",
    "    # training the linear svm\n",
    "    lin_svm=SVC(kernel='linear')\n",
    "    lin_svm.fit(tr_fea.T,tr_label.T)\n",
    "\n",
    "    # using the trained svm on test data\n",
    "    prediction=lin_svm.predict(te_fea.T)\n",
    "\n",
    "    # calculating the test accuracy\n",
    "    Accuracy=accuracy_score(prediction,te_label.T)\n",
    "\n",
    "    print(\"On training the model on first\",x,\"training data points\")\n",
    "    print(\"The test accuracy of linear SVM is:\",Accuracy)   \n",
    "    print(\"Number of support vectors:\",lin_svm.support_vectors_.shape[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf0641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing back the complete training data into the old varible\n",
    "tr_label=TR_label\n",
    "tr_fea=TR_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9bd670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For c = 0.0001\n",
      "The training error of polynomial SVM with degree 2: 0.25368353619474693\n",
      "The training error of polynomial SVM with degree 5: 0.018577834721332454\n",
      "The number of support vectors of polynomial SVM with degree 2: 1112\n",
      "The number of support vectors of polynomial SVM with degree 5: 188\n",
      "The testing error of polynomial SVM with degree 2: 0.2570754716981132\n",
      "The testing error of polynomial SVM with degree 5: 0.028301886792452824\n",
      "\n",
      "For c = 0.001\n",
      "The training error of polynomial SVM with degree 2: 0.014093529788597015\n",
      "The training error of polynomial SVM with degree 5: 0.006406149903907754\n",
      "The number of support vectors of polynomial SVM with degree 2: 456\n",
      "The number of support vectors of polynomial SVM with degree 5: 72\n",
      "The testing error of polynomial SVM with degree 2: 0.02594339622641506\n",
      "The testing error of polynomial SVM with degree 5: 0.018867924528301883\n",
      "\n",
      "For c = 0.01\n",
      "The training error of polynomial SVM with degree 2: 0.005124919923126248\n",
      "The training error of polynomial SVM with degree 5: 0.004484304932735439\n",
      "The number of support vectors of polynomial SVM with degree 2: 132\n",
      "The number of support vectors of polynomial SVM with degree 5: 34\n",
      "The testing error of polynomial SVM with degree 2: 0.018867924528301883\n",
      "The testing error of polynomial SVM with degree 5: 0.01650943396226412\n",
      "\n",
      "For c = 1\n",
      "The training error of polynomial SVM with degree 2: 0.004484304932735439\n",
      "The training error of polynomial SVM with degree 5: 0.0038436899423446302\n",
      "The number of support vectors of polynomial SVM with degree 2: 28\n",
      "The number of support vectors of polynomial SVM with degree 5: 25\n",
      "The testing error of polynomial SVM with degree 2: 0.021226415094339646\n",
      "The testing error of polynomial SVM with degree 5: 0.02358490566037741\n",
      "\n",
      "So, the statement 1 is FALSE\n",
      "So, the statement 2 is TRUE\n",
      "So, the statement 3 is FALSE\n",
      "So, the statement 4 is FALSE\n"
     ]
    }
   ],
   "source": [
    "# the values of C in which we are interested in\n",
    "C_par=[0.0001, 0.001, 0.01, 1]\n",
    "\n",
    "# looping the same code for different values of C\n",
    "for x in C_par:\n",
    "    \n",
    "    # training the polynomial svm with degree 2\n",
    "    poly_svm1=SVC(kernel='poly', degree=2, C=x, coef0=1)\n",
    "    poly_svm1.fit(tr_fea.T,tr_label.T)\n",
    "\n",
    "    # training the polynomial svm with degree 5\n",
    "    poly_svm2=SVC(kernel='poly', degree=5, C=x, coef0=1)\n",
    "    poly_svm2.fit(tr_fea.T,tr_label.T)\n",
    "\n",
    "    # using the trained svm1 on training data\n",
    "    tr_prediction1=poly_svm1.predict(tr_fea.T)\n",
    "\n",
    "    # using the trained svm2 on training data\n",
    "    tr_prediction2=poly_svm2.predict(tr_fea.T)\n",
    "\n",
    "    # calculating the training accuracy of svm1\n",
    "    tr_Accuracy1=accuracy_score(tr_prediction1,tr_label.T)\n",
    "\n",
    "    # calculating the training accuracy of svm2\n",
    "    tr_Accuracy2=accuracy_score(tr_prediction2,tr_label.T)\n",
    "\n",
    "    # using the trained svm1 on testing data\n",
    "    te_prediction1=poly_svm1.predict(te_fea.T)\n",
    "\n",
    "    # using the trained svm2 on testing data\n",
    "    te_prediction2=poly_svm2.predict(te_fea.T)\n",
    "\n",
    "    # calculating the test accuracy of svm1\n",
    "    te_Accuracy1=accuracy_score(te_prediction1,te_label.T)\n",
    "\n",
    "    # calculating the test accuracy of svm2\n",
    "    te_Accuracy2=accuracy_score(te_prediction2,te_label.T)\n",
    "\n",
    "    print(\"For c =\",x)\n",
    "    print(\"The training error of polynomial SVM with degree 2:\",1-tr_Accuracy1)   \n",
    "    print(\"The training error of polynomial SVM with degree 5:\",1-tr_Accuracy2)\n",
    "    print(\"The number of support vectors of polynomial SVM with degree 2:\",poly_svm1.support_vectors_.shape[0])   \n",
    "    print(\"The number of support vectors of polynomial SVM with degree 5:\",poly_svm2.support_vectors_.shape[0])\n",
    "    print(\"The testing error of polynomial SVM with degree 2:\",1-te_Accuracy1)   \n",
    "    print(\"The testing error of polynomial SVM with degree 5:\",1-te_Accuracy2)\n",
    "    print()\n",
    "    \n",
    "print(\"So, the statement 1 is FALSE\")\n",
    "print(\"So, the statement 2 is TRUE\")\n",
    "print(\"So, the statement 3 is FALSE\")\n",
    "print(\"So, the statement 4 is FALSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db65b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For c = 0.01\n",
      "The training error of rbf SVM: 0.0038436899423446302\n",
      "The number of support vectors of rbf SVM: 406\n",
      "The number of support vectors of rbf SVM: 406\n",
      "The testing error of rbf SVM: 0.02358490566037741\n",
      "\n",
      "For c = 1\n",
      "The training error of rbf SVM: 0.004484304932735439\n",
      "The number of support vectors of rbf SVM: 31\n",
      "The number of support vectors of rbf SVM: 31\n",
      "The testing error of rbf SVM: 0.021226415094339646\n",
      "\n",
      "For c = 100\n",
      "The training error of rbf SVM: 0.0032030749519538215\n",
      "The number of support vectors of rbf SVM: 22\n",
      "The number of support vectors of rbf SVM: 22\n",
      "The testing error of rbf SVM: 0.018867924528301883\n",
      "\n",
      "For c = 10000.0\n",
      "The training error of rbf SVM: 0.002562459961563124\n",
      "The number of support vectors of rbf SVM: 19\n",
      "The number of support vectors of rbf SVM: 19\n",
      "The testing error of rbf SVM: 0.02358490566037741\n",
      "\n",
      "For c = 1000000.0\n",
      "The training error of rbf SVM: 0.0006406149903908087\n",
      "The number of support vectors of rbf SVM: 17\n",
      "The number of support vectors of rbf SVM: 17\n",
      "The testing error of rbf SVM: 0.02358490566037741\n",
      "\n",
      "So, the lowest training error (0.0006406149903908087) occurs at C=1000000(1e6)\n",
      "So, the lowest testing error (0.018867924528301883) occurs at C=100(1e2)\n"
     ]
    }
   ],
   "source": [
    "# the values of C in which we are interested in\n",
    "C_par=[0.01, 1, 100, 1e4, 1e6]\n",
    "\n",
    "# looping the same code for different values of C\n",
    "for x in C_par:\n",
    "    \n",
    "    # training the svm with rbf kernel\n",
    "    rbf_svm1=SVC(kernel='rbf', C=x, gamma=1)\n",
    "    rbf_svm1.fit(tr_fea.T,tr_label.T)\n",
    "\n",
    "    # training the polynomial svm with degree 5\n",
    "    rbf_svm2=SVC(kernel='rbf', C=x, gamma=1)\n",
    "    rbf_svm2.fit(tr_fea.T,tr_label.T)\n",
    "\n",
    "    # using the trained svm1 on training data\n",
    "    tr_prediction1=rbf_svm1.predict(tr_fea.T)\n",
    "\n",
    "    # using the trained svm2 on training data\n",
    "    tr_prediction2=rbf_svm2.predict(tr_fea.T)\n",
    "\n",
    "    # calculating the training accuracy of svm1\n",
    "    tr_Accuracy1=accuracy_score(tr_prediction1,tr_label.T)\n",
    "\n",
    "    # calculating the training accuracy of svm2\n",
    "    tr_Accuracy2=accuracy_score(tr_prediction2,tr_label.T)\n",
    "\n",
    "    # using the trained svm1 on testing data\n",
    "    te_prediction1=rbf_svm1.predict(te_fea.T)\n",
    "\n",
    "    # using the trained svm2 on testing data\n",
    "    te_prediction2=rbf_svm2.predict(te_fea.T)\n",
    "\n",
    "    # calculating the test accuracy of svm1\n",
    "    te_Accuracy1=accuracy_score(te_prediction1,te_label.T)\n",
    "\n",
    "    # calculating the test accuracy of svm2\n",
    "    te_Accuracy2=accuracy_score(te_prediction2,te_label.T)\n",
    "\n",
    "    print(\"For c =\",x)\n",
    "    print(\"The training error of rbf SVM:\",1-tr_Accuracy1)   \n",
    "    print(\"The number of support vectors of rbf SVM:\",rbf_svm1.support_vectors_.shape[0])   \n",
    "    print(\"The number of support vectors of rbf SVM:\",rbf_svm2.support_vectors_.shape[0])\n",
    "    print(\"The testing error of rbf SVM:\",1-te_Accuracy1)   \n",
    "    print()\n",
    "    \n",
    "print(\"So, the lowest training error (0.0006406149903908087) occurs at C=1000000(1e6)\")\n",
    "print(\"So, the lowest testing error (0.018867924528301883) occurs at C=100(1e2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6b324",
   "metadata": {},
   "source": [
    "## Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da774b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading training features\n",
    "with open(\"gisette_train.data\") as f:\n",
    "    tr_fea = [list(line) for line in csv.reader(f, delimiter=\" \")]\n",
    "\n",
    "# typecasting data for easier calculations\n",
    "tr_fea = np.array(tr_fea)\n",
    "tr_fea=tr_fea.T\n",
    "tr_fea=tr_fea[:-1]\n",
    "tr_fea = np.array(tr_fea,dtype=int)\n",
    "\n",
    "# Loading training labels\n",
    "with open(\"gisette_train.labels\") as f:\n",
    "    tr_labels = [list(line) for line in csv.reader(f, delimiter=\" \")]\n",
    "\n",
    "# typecasting data for easier calculations\n",
    "tr_labels = np.array(tr_labels,dtype=int)\n",
    "\n",
    "# converting to 1d array\n",
    "tr_labels=(tr_labels.T).flatten()\n",
    "\n",
    "# Loading validation features\n",
    "with open(\"gisette_valid.data\") as f:\n",
    "    va_fea = [list(line) for line in csv.reader(f, delimiter=\" \")]\n",
    "\n",
    "# typecasting data for easier calculations\n",
    "va_fea = np.array(va_fea)\n",
    "va_fea=va_fea.T\n",
    "va_fea=va_fea[:-1]\n",
    "va_fea = np.array(va_fea,dtype=int)\n",
    "\n",
    "# Loading validation labels\n",
    "with open(\"gisette_valid.labels\") as f:\n",
    "    va_labels = [list(line) for line in csv.reader(f, delimiter=\" \")]\n",
    "\n",
    "# typecasting data for easier calculations\n",
    "va_labels = np.array(va_labels,dtype=int)\n",
    "\n",
    "# converting to 1d array\n",
    "va_labels=(va_labels.T).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9591304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Linear SVM\n",
      "Number of support vectors: 1084\n",
      "Training error: 0.0\n",
      "Validation error: 0.02400000000000002\n"
     ]
    }
   ],
   "source": [
    "# training the linear svm\n",
    "lin_svm=SVC(kernel='linear')\n",
    "lin_svm.fit(tr_fea.T,tr_labels)\n",
    "print(\"For Linear SVM\")\n",
    "print(\"Number of support vectors:\",lin_svm.support_vectors_.shape[0])\n",
    "\n",
    "# using the trained svm on training data\n",
    "prediction=lin_svm.predict(tr_fea.T)\n",
    "\n",
    "# calculating the training accuracy\n",
    "Accuracy=accuracy_score(prediction,tr_labels)\n",
    "\n",
    "# printing the training error\n",
    "print(\"Training error:\",1-Accuracy)\n",
    "\n",
    "# using the trained svm on validation data\n",
    "prediction=lin_svm.predict(va_fea.T)\n",
    "\n",
    "# calculating the validation accuracy\n",
    "Accuracy=accuracy_score(prediction,va_labels)\n",
    "\n",
    "# printing the validation error\n",
    "print(\"Validation error:\",1-Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba08de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rbf kernel SVM\n",
      "Number of support vectors: 6000\n",
      "Training error: 0.0\n",
      "Validation error: 0.5\n"
     ]
    }
   ],
   "source": [
    "# training the rbf kernel svm\n",
    "rbf_svm=SVC(kernel='rbf',gamma=0.001)\n",
    "rbf_svm.fit(tr_fea.T,tr_labels)\n",
    "print(\"For rbf kernel SVM\")\n",
    "print(\"Number of support vectors:\",rbf_svm.support_vectors_.shape[0])\n",
    "\n",
    "# using the trained svm on training data\n",
    "prediction=rbf_svm.predict(tr_fea.T)\n",
    "\n",
    "# calculating the training accuracy\n",
    "Accuracy=accuracy_score(prediction,tr_labels)\n",
    "\n",
    "# printing the training error\n",
    "print(\"Training error:\",1-Accuracy)\n",
    "\n",
    "# using the trained svm on validation data\n",
    "prediction=rbf_svm.predict(va_fea.T)\n",
    "\n",
    "# calculating the validation accuracy\n",
    "Accuracy=accuracy_score(prediction,va_labels)\n",
    "\n",
    "# printing the validation error\n",
    "print(\"Validation error:\",1-Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68282ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For polynomial kernel SVM\n",
      "Number of support vectors: 1332\n",
      "Training error: 0.0004999999999999449\n",
      "Validation error: 0.020000000000000018\n"
     ]
    }
   ],
   "source": [
    "# training the polynomial kernel svm\n",
    "poly_svm=SVC(kernel='poly',degree=2,coef0=1)\n",
    "poly_svm.fit(tr_fea.T,tr_labels)\n",
    "print(\"For polynomial kernel SVM\")\n",
    "print(\"Number of support vectors:\",poly_svm.support_vectors_.shape[0])\n",
    "\n",
    "# using the trained svm on training data\n",
    "prediction=poly_svm.predict(tr_fea.T)\n",
    "\n",
    "# calculating the training accuracy\n",
    "Accuracy=accuracy_score(prediction,tr_labels)\n",
    "\n",
    "# printing the training error\n",
    "print(\"Training error:\",1-Accuracy)\n",
    "\n",
    "# using the trained svm on validation data\n",
    "prediction=poly_svm.predict(va_fea.T)\n",
    "\n",
    "# calculating the validation accuracy\n",
    "Accuracy=accuracy_score(prediction,va_labels)\n",
    "\n",
    "# printing the validation error\n",
    "print(\"Validation error:\",1-Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a11ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearly, both linear kernel and rbf kernel SVMs give lowest training errors. Both have training error of 0.\n"
     ]
    }
   ],
   "source": [
    "print(\"Clearly, both linear kernel and rbf kernel SVMs give lowest training errors. Both have training error of 0.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
